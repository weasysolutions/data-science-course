{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuarta Lección "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark está escrito en el lenguaje de programación Scala que compila el código del programa en un código de bytes para la Máquina Virtual de Java para el procesamiento de big data de spark. La comunidad de código abierto ha desarrollado una maravillosa utilidad para el procesamiento de big data de spark python conocido como PySpark.\n",
    "\n",
    "PySpark ayuda a los científicos de datos a interactuar con los Datos Resilientes Distribuidos (Resilient Distributed Datasets) en apache spark y python. Py4J es una biblioteca popular integrada en PySpark que permite a Python interactuar dinámicamente con objetos JVM (RDD).\n",
    "\n",
    "La mayoría de las operaciones de RDD son perezosas. Un RDD es como una descripción de una serie de operaciones. Un RDD no son datos. Las operaciones RDD que requieren la observación del contenido de los datos no pueden ser perezosas. (Estas son llamadas acciones.) Por ejemplo, la funcion count() que enumera el numero de elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el procesamiento paralelo, Apache Spark utiliza variables compartidas. Una copia de la variable compartida va a cada nodo del clúster cuando el controlador envía una tarea al ejecutor en el clúster, de modo que pueda usarse para realizar tareas.\n",
    "\n",
    "Apache Spark admite dos tipos de variables compartidas:\n",
    "\n",
    " __Emisión__\n",
    "\n",
    "__Acumulación__\n",
    "\n",
    "Vamos a entenderlos en detalle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emisión\n",
    "Las variables de difusión se utilizan para guardar la copia de datos en todos los nodos. Esta variable se almacena en caché en todas las máquinas y no se envía en máquinas con tareas. El siguiente bloque de código tiene los detalles de una clase de transmisión para PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "from pyspark import SparkContext \n",
    "sc = SparkContext(\"local\", \"Broadcast app\") \n",
    "words_new = sc.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"]) \n",
    "data = words_new.value \n",
    "print(\"Stored data -> %s\" % (data) )\n",
    "elem = words_new.value[2] \n",
    "print(\"Printing a particular element in RDD -> %s\" % (elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos el siguiente ejemplo de uso de SparkConf en un programa PySpark. En este ejemplo, estamos configurando el nombre de la aplicación de la chispa como aplicación PySpark y estableciendo la URL maestra para una aplicación de la chispa en → spark: // master: 7077.\n",
    "\n",
    "El siguiente bloque de código tiene las líneas, cuando se agregan en el archivo Python, establece las configuraciones básicas para ejecutar una aplicación PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"spark://master:7077\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "finddistance = \"/home/hadoop/examples_pyspark/finddistance.R\"\n",
    "finddistancename = \"finddistance.R\"\n",
    "sc = SparkContext(\"local\", \"SparkFile App\")\n",
    "sc.addFile(finddistance)\n",
    "print \"Absolute Path -> %s\" % SparkFiles.get(finddistancename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<font color=blue>\n",
    "    _Objetivo del curso_: Aprender las diferentes fases del Aprendizaje de Datos (ML) a traves de  ejemplos reales. Esto con el fin de obtener una idea clara de lo que significa la ciencia de datos.\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
