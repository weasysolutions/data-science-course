{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tercera Lección "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### _<font color=blue>  Objetivo de la lección: </font>_\n",
    "\n",
    "<font color=blue>\n",
    " Aprender las diferentes fases del Aprendizaje de Datos (ML): Ingeniería de características y Modelado de Datos.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\">El flujo de trabajo en un proyecto de ciencia de datos </h2>\n",
    "\n",
    "El conjunto de  lineamientos específicos ('ducto') en un proyecto de  ciencia de datos varía dependiendo de la naturaleza del mismo. Aquí se utiliza el ducto:\n",
    "\n",
    "### Análisis exploratorio de los datos\n",
    "\n",
    "### Ingeniería de Características\n",
    "\n",
    "### Modelado de datos\n",
    "\n",
    " ***\n",
    " Específicamente, en esta leccion se tratan los dos últimos puntos\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_Ajuste el estilo de este libro de trabajo para tener gráficas centradas_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análisis exploratorio de los datos\n",
    "Los resultados de la lección pasada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Características\n",
    "Los nombres de las características del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "caracteristicas =  ['a:radio','b:textura','c:perimetro', \\\n",
    "             'd:area', 'e:suavidad','f:compactes', \\\n",
    "             'g:concavidad', 'h:puntos_concavos','i:simetria',\\\n",
    "             'j:dimension_fractal']\n",
    "\n",
    "columnas = ['ID', 'Diagnostico'] + \\\n",
    "              list(map(lambda x: x[2:] + '_promedio', caracteristicas)) + \\\n",
    "              list(map(lambda x: x[2:] + '_error', caracteristicas)) +  \\\n",
    "              list(map(lambda x: x[2:] + '_peor', caracteristicas))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  Extracción \n",
    "Importe la librería **Pandas** para extraer el conjunto de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../data/wisconsin.csv', names = columnas);\n",
    "data = data.reset_index().drop(columns =['index'])\n",
    "data = data.drop(columns='ID')\n",
    "\n",
    "data['Benigno'] = (data['Diagnostico']=='B')*1\n",
    "data['Maligno'] = 1 - data['Benigno']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La ingeniería de características es el proceso de utilizar el conocimiento de dominio de los datos para crear características que hacen que los algoritmos de aprendizaje automático funcionen. La ingeniería de características es fundamental para la aplicación del aprendizaje automático.\n",
    "\n",
    "## Ingeniería de Características\n",
    "\n",
    " - Datos Categóricos\n",
    " - Divsion del conjunto de datos\n",
    " - Escalamiento de los atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ingeniería de Características\n",
    "\n",
    "### Datos Categóricos\n",
    "\n",
    "Los datos categóricos son variables que contienen valores de etiqueta en lugar de valores numéricos. El número de valores posibles a menudo se limita a un conjunto fijo.\n",
    "\n",
    "Por ejemplo, los usuarios normalmente se describen por país, género, grupo de edad, etc.\n",
    "\n",
    "De hecho ya se han  convertido los atributos del _Diagnóstico_ a valores numéricos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Maligno</th>\n",
       "      <th>Benigno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Maligno  Benigno\n",
       "433        1        0\n",
       "157        0        1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Maligno','Benigno']].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las columnas _Maligno_ y _Benigno_ son redundates. El conocimiento de una, automaticamente nos define la otra. Elimine una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns = 'Maligno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En este punto la columna _Diagnostico_ tambien se vuelve redundante, por lo que también se elimina:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns = 'Diagnostico')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### División del conjunto de datos \n",
    "\n",
    "Los datos que utiliza, generalmente se dividen en datos de _entrenamiento_ y datos de _prueba_. \n",
    "El conjunto de entrenamiento contiene _un_ resultado conocido y \n",
    "el modelo aprende sobre estos datos para generalizarse a otros datos más adelante. \n",
    "Se cuenta con el conjunto de datos de prueba (o subconjunto) para probar la predicción del modelo \n",
    "en este subconjunto.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "Primero, separe el conjunto total de diagnósticos (presentado por la letra _Y_) del resto de los atributos (representados por la letra _X_)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0:29].values\n",
    "Y = data.iloc[:, 30].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Separe el conjunto de entrenamiento y el conjunto prueba usando la biblioteca **SciKit-Learn**. En especifico, use el método 'train_test_split':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El parámetro _test_\\__size_ que aparece en el método\n",
    "\n",
    "_train_\\__test_\\__split()_\n",
    "\n",
    "representa el porcentaje de los datos _(X,Y)_  que serán usados como datos de prueba. En este caso se ha usado el veinticinco porciento. El parámetro _random_\\__state_ sirve para establecer la semilla del algoritmo aleatorio que selecciona los conjuntos de datos \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Escalamiento de los atributos \n",
    "\n",
    "La mayoría de las veces, un conjunto de datos contendrá características altamente variables en su magnitudes, unidades y rango. Por ejemplo note la gran disparidad que existe entre las columnas 'dimension_fractal' y 'radio_promedio':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_promedio</th>\n",
       "      <th>dimension_fractal_promedio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.062798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.007060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.049960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.061540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.066120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.097440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       area_promedio  dimension_fractal_promedio\n",
       "count     569.000000                  569.000000\n",
       "mean      654.889104                    0.062798\n",
       "std       351.914129                    0.007060\n",
       "min       143.500000                    0.049960\n",
       "25%       420.300000                    0.057700\n",
       "50%       551.100000                    0.061540\n",
       "75%       782.700000                    0.066120\n",
       "max      2501.000000                    0.097440"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['area_promedio','dimension_fractal_promedio']].describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " Asi como la cercania o lejania entre dos ciudades distintas se establece con la distancia que los separa, en estadística también se tiene el concepto de distancia entre los datos _medidos_ con los datos del _modelo teórico_. De hecho, la noción de distancia en estadística es la misma que la que hay para dos ciudades: la medida Euclideana.\n",
    " Con ella, se puede decir si nuestras predicciones del modelo están cerca o lejos de las predicciones de los datos medidos. Es un poco descabellado comparar distancias de ciudades con las distancias entre dos hormigas en un mismo hormiguero, pero los ejemplos del 'area_promedio' y 'dimension_fractal_promedio' arriba expuestos  están demostrando que se encuentra en esta situacion.\n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_<div style=\"text-align: center\">  Es deseable llevar todas las características al mismo nivel de magnitudes (normalizar los datos). Esto se puede lograr mediante la escala. Esto significa transformar todos los datos a una misma  escala específica, \n",
    "como  de 0 a 100 o de  0 a 1.  </div>_\n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para normalizar los datos, use el método _StandardScaler_ de la biblioteca **sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Modelado de datos\n",
    "\n",
    "El modelado de datos en ingeniería de software representa el proceso de crear un modelo de datos para un sistema de información mediante la aplicación de las técnicas de Aprendizaje de Datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el área del aprendizaje de maquinas, las predicciones se diferencían  en dos tipos. Por un lado, las predicciones que toman valores en el  continúo (en la recta real), se les llama _Regresiones_. Por ejemplo, el tiempo que tarda un avion en cruzar el continente. Mientras que las predicciones que involucran solo un número finito de predicciones (discreto o categórico), se les conoce como _clasificaciones_. Por ejemplo un conjunto de tres colores, el día o la noche, etc.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En el caso aquí tratado se desea predecir si el cáncer es maligno o benigno, entonces se usan _algoritmos de clasificación para el aprendizaje supervisado_ \n",
    "***\n",
    "La palabra supervisado, aquí significa que se conoce la etiqueta final que se desea predecir: _Benigno_ o _Maligno_. Las etiquetas _Benigno_  y _Maligno_  que se usan para entrenar a la maquina, no deben ser llamadas  _predicciones_ sino _datos de salida_: toda predicción es un dato de salida pero no al reves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clasificación in ML\n",
    "EN SUMA:\n",
    "\n",
    "En aprendizaje automático y estadística, la clasificación consiste identificar cuál de un conjunto de categorías (subpoblaciones) pertenece una nueva observación, sobre la base de un conjunto de entrenamiento de datos que contienen observaciones (o instancias) cuya categoría de miembro es conocida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmos de ML\n",
    "\n",
    "Los siguientes nombres de algoritmos de clasificación en Aprendizaje Automático, son de los más populares en el mundo de la ciencia de datos:\n",
    "\n",
    "1. Regresión Logística\n",
    "\n",
    "2. Vecino Más Cercano\n",
    "\n",
    "3. Soporte de Máquinas de Vectores\n",
    "\n",
    "4. Kernel SVM\n",
    "\n",
    "5. Naïve Bayes\n",
    "\n",
    "6. Árbol de Decisión\n",
    "\n",
    "7. Bosques al Azar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La biblioteca **sklearn** incluye todos estos métodos. Se importarán las correspondientes sub-librerias .\n",
    "\n",
    "Se ejemplifica a detalle el análisis de los datos para el caso de  la  _regresión logística_. Despues se introducen los demás algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### REGRESIÓN LOGÍSTICA.\n",
    "\n",
    "En su forma más sencilla (la cual es la que necesitamos aqui), la regresión logística modela las variables de salida 'Benigno y 'Maligno', en este caso se habla de una _regresión logística binaria_\n",
    "\n",
    "\n",
    "El resultado de un paciente no influyen en los demás, entonces las pruebas de los pacientes constituyen un\n",
    "__Ensayo de Bernoulli:__\n",
    "Ensayos repetidos independientes de un experimento con exactamente dos resultados posibles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Regresión Logística vs Regresión Ordinaria\n",
    "La regresión logística difiere de la regresión ordinaria en que la segunda regresa valores continuos. En la regresión logísitica, la simplicidad de la regresión lineal es usada, pero tiene que ser adicionada con  una forma de convertir una variable binaria en una continua que pueda tomar cualquier valor real (negativo o positivo). \n",
    "\n",
    "Para ello se definen  la _frontera de decisión_ (_boundary decision_). \n",
    "\n",
    "Esta predicción categórica se puede basar en las probabilidades calculadas de éxito, y las probabilidades pronosticadas por encima de un valor de corte elegido se traducen en una predicción de éxito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Funciones de Activación\n",
    "\n",
    "Computacionalmente es mejor aproximar probabilidades de valores categóricos con funciones continuas conocidas como funciones de activación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo\n",
    "Las siguientes dos funciones, son funciones de activacion y se conocen como las funciones _sigmoid_ y _tanh_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sigmoid(x):  \n",
    "    return  np.array([1 / (1 + np.exp(-y)) for y in x])\n",
    "\n",
    "def tanh(x):\n",
    "    return np.array([np.tanh(y) for y in x])\n",
    "\n",
    "t = np.linspace(-4,4,400)\n",
    "\n",
    "a = 4*sigmoid(t) - 2\n",
    "b = 1*tanh(t) + 0\n",
    "\n",
    "plt.plot(t,a,t,b)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A continuación cargamos el método de regresión logística de **skit-learn** que hará el _ajuste_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Usando el algoritmo de regresion logistica para entrenar el conjunto\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0,solver='liblinear')\n",
    "\n",
    "#Una vez cargado el clasificador, hacemos el ajuste\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Teniendo el _clasificador_ entrenado, toca el turno de generar predicciones (_respuestas_) para el conjunto de prueba (_predictór_ ) X_test,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para verificar la precisión de la predicción importe el método de confusión de matriz de métricas. La matríz de confusión es una forma de tabular el número de clasificaciones erróneas, es decir, el número de clases predichas (Y_pred) que terminaron en un contenedor de clasificación incorrecto basado en las clases verdaderas (Y_train)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matríz de Error o Matríz de Confusión \n",
    "\n",
    "En el análisis predictivo, una tabla de confusión (o matríz de confusión), es una tabla con dos filas y dos columnas que informa el número de _falsos positivos_, _falsos negativos_, _verdaderos positivos_ y _verdaderos negativos_.\n",
    "\n",
    "Cada fila de la matríz representa las instancias en una clase predicha, mientras que cada columna representa las instancias en una clase real (o viceversa). En este caso, tenemos algo como:\n",
    "\n",
    "\n",
    "| Prediccion/Real | Maligno | Benigno |\n",
    "| --- | --- | --- |\n",
    "| Maligno | $VP$ | $FP$|\n",
    "| Benigno | $FN$ | $VN$ |\n",
    "\n",
    "$VP$ = Número de  Verdadero Positivo\n",
    "\n",
    "$FP$ = Número de Falso Positivo\n",
    "\n",
    "$VN$ = Número de Verdadero Negativo\n",
    "\n",
    "$FN$ = Número de Falso Negativo\n",
    "\n",
    "\n",
    "**condición positiva (P):**\n",
    "El número de casos positivos reales en los datos.\n",
    "\n",
    "**condición negativa (N):**\n",
    "El número de casos negativos reales en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Importe el método de métrica que contiene la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 143 casos probados nos arrojan la siguiente matriz de confusión: \n",
      " [[49  4]\n",
      " [ 4 86]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "#La matriz de confusión\n",
    "cm = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "print('Los', len(Y_test), 'casos probados', \\\n",
    "      'nos arrojan la siguiente matriz de confusión: \\n', cm \n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exactitud \n",
    "\n",
    "Para el caso de errores categoricos (como en nuestro caso), esta simple clasificacion puede ser engañosa, por que las muestras pueden estar cargadas hacia un solo caso. Es recomendable apoyarnos en la _Exactitud Matematica_, la cual viene dada simplemente por la relacion:\n",
    "***\n",
    "$\\textit{Exactitud} = \\frac{\\text{Número de Predicciones Verdaderas}}{\\text{Número Total de Predicciones}}$\n",
    "\n",
    "\n",
    "Para nuestro caso de predicciones categóricas binarias:\n",
    "\n",
    "\n",
    "$\\textit{Exactitud} = \\frac{\\text{VP+VN}}{\\text{VP+VN+FP+FN}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "El método _metrics.accuracy_\\__score_ lo calcula por usted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9440559440559441"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(Y_test, Y_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Toca el turno de repetir el mismo ejercicio sobre las diferentes rutinas de Machine Learning que **sklearn** provee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificadores = {}\n",
    "\n",
    "#Para el conjunto de entrenamiento usamos:\n",
    "\n",
    "#Usando el Algoritmo de Regresion Logistica \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clasificadores['Regresion Logistica'] = LogisticRegression(random_state = 0, solver='liblinear')\n",
    "clasificadores['Regresion Logistica'].fit(X_train, Y_train)\n",
    "\n",
    "#Usando el Algoritmo de K Vecinos Próximos \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clasificadores['K Vecinos Próximos'] = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "clasificadores['K Vecinos Próximos'].fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Usando el Algoritmo de Máquinas de Soporte Vectorial \n",
    "from sklearn.svm import SVC\n",
    "clasificadores['Máquinas de Soporte Vectorial'] = SVC(kernel = 'linear', random_state = 0)\n",
    "clasificadores['Máquinas de Soporte Vectorial'].fit(X_train, Y_train)\n",
    "\n",
    "#Usando el Algoritmo de Núcleo SVM \n",
    "from sklearn.svm import SVC\n",
    "clasificadores['Núcleo SVM '] = SVC(kernel = 'rbf', random_state = 0)\n",
    "clasificadores['Núcleo SVM '].fit(X_train, Y_train)\n",
    "\n",
    "#Usando el Algoritmo Bayessiano Ingenuo\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clasificadores['Bayes Ingenuo'] = GaussianNB()\n",
    "clasificadores['Bayes Ingenuo'].fit(X_train, Y_train)\n",
    "\n",
    "#Usando el Arbol de Decisión \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clasificadores['Arbol de Decisión'] = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "clasificadores['Arbol de Decisión'].fit(X_train, Y_train)\n",
    "\n",
    "#Usando el Bosque Aleatorio \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clasificadores['Bosque Aleatorio'] = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "clasificadores['Bosque Aleatorio'].fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "exactitudes = {}\n",
    "for clasificador in clasificadores:\n",
    "    \n",
    "    Y_pred = clasificadores[clasificador].predict(X_test) \n",
    "    \n",
    "    exactitudes[clasificador] = metrics.accuracy_score(Y_test, Y_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El clasificador Regresion Logistica es exacto hasta un 94.41 %\n",
      "El clasificador K Vecinos Próximos es exacto hasta un 95.80 %\n",
      "El clasificador Máquinas de Soporte Vectorial es exacto hasta un 96.50 %\n",
      "El clasificador Núcleo SVM  es exacto hasta un 96.50 %\n",
      "El clasificador Bayes Ingenuo es exacto hasta un 92.31 %\n",
      "El clasificador Arbol de Decisión es exacto hasta un 95.10 %\n",
      "El clasificador Bosque Aleatorio es exacto hasta un 95.80 %\n"
     ]
    }
   ],
   "source": [
    "for clasificador in exactitudes:\n",
    "    print('El clasificador', clasificador, 'es exacto hasta un',  '%.2f' % (100*exactitudes[clasificador]),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusiones \n",
    "\n",
    "Se ha completado el ducto de ciencia de datos para el caso del conjunto de datos del cáncer de mama de Wisconsin"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
